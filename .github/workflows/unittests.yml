name: Python Tests

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main, develop ]
  schedule:
    # Run tests weekly on Monday at 9 AM UTC
    - cron: '0 9 * * 1'

jobs:
  test:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: [3.9, '3.10', '3.11']
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ matrix.python-version }}
        cache: 'pip'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install pytest pytest-cov flake8 black mypy
    
    - name: Check code formatting with black
      run: |
        black --check src/ tests/ dashboard/
    
    - name: Lint with flake8
      run: |
        flake8 src/ tests/ dashboard/ --count --select=E9,F63,F7,F82 --show-source --statistics
        flake8 src/ tests/ dashboard/ --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics
    
    - name: Type check with mypy
      run: |
        mypy --ignore-missing-imports src/
    
    - name: Test data directory structure
      run: |
        # Check required directories exist
        if [ ! -d "data/raw" ]; then
          echo "âŒ data/raw directory does not exist"
          exit 1
        fi
        if [ ! -d "data/processed" ]; then
          echo "âŒ data/processed directory does not exist"
          exit 1
        fi
        echo "âœ… Data directories exist"
    
    - name: Check for required files
      run: |
        # Create minimal required files if they don't exist (for CI/CD)
        if [ ! -f "data/raw/ethiopia_fi_unified_data.csv" ]; then
          echo "âš ï¸  Creating sample data file for testing..."
          mkdir -p data/raw
          echo "record_type,pillar,indicator,indicator_code,value_numeric,observation_date,source_name,confidence" > data/raw/ethiopia_fi_unified_data.csv
          echo "observation,access,Account Ownership Rate,ACC_OWNERSHIP,49.0,2024-01-01,Global Findex,high" >> data/raw/ethiopia_fi_unified_data.csv
        fi
        
        if [ ! -f "data/raw/reference_codes.csv" ]; then
          echo "âš ï¸  Creating sample reference codes for testing..."
          echo "field,code,description" > data/raw/reference_codes.csv
          echo "record_type,observation,Measured value" >> data/raw/reference_codes.csv
          echo "record_type,event,Historical event" >> data/raw/reference_codes.csv
        fi
        
        # Check other required files
        if [ ! -f "requirements.txt" ]; then
          echo "âŒ requirements.txt does not exist"
          exit 1
        fi
        
        if [ ! -f "dashboard/app.py" ]; then
          echo "âŒ dashboard/app.py does not exist"
          exit 1
        fi
        
        echo "âœ… All required files exist"
    
    - name: Run unit tests with pytest
      run: |
        python -m pytest tests/ -v --cov=src --cov-report=xml --cov-report=html
    
    - name: Test data loader module
      run: |
        python -c "
        import sys
        sys.path.append('src')
        try:
            from data_loader import DataLoader
            loader = DataLoader()
            data, ref_codes = loader.load_data()
            print('âœ… DataLoader module loaded successfully')
            print(f'   Data shape: {data.shape if hasattr(data, \"shape\") else \"No shape\"}')
            print(f'   Reference codes shape: {ref_codes.shape if hasattr(ref_codes, \"shape\") else \"No shape\"}')
        except Exception as e:
            print(f'âŒ DataLoader module failed: {e}')
            sys.exit(1)
        "
    
    - name: Test dashboard initialization
      run: |
        python -c "
        import sys
        sys.path.append('src')
        sys.path.append('dashboard')
        try:
            # Test if dashboard modules can be imported
            import app
            print('âœ… Dashboard modules can be imported')
            
            # Test if main dashboard class exists
            if hasattr(app, 'FinancialInclusionDashboard'):
                print('âœ… FinancialInclusionDashboard class exists')
            else:
                print('âš ï¸  FinancialInclusionDashboard class not found')
                
        except Exception as e:
            print(f'âŒ Dashboard initialization failed: {e}')
            sys.exit(1)
        "
    
    - name: Create test data samples
      run: |
        # Create test processed data directory
        mkdir -p data/processed
        mkdir -p models
        
        # Create sample enriched data for testing
        echo "Creating sample enriched data..."
        python -c "
        import pandas as pd
        from datetime import datetime
        
        # Create sample enriched data
        enriched_data = pd.DataFrame({
            'record_type': ['observation', 'observation', 'event', 'impact_link'],
            'pillar': ['access', 'usage', None, 'access'],
            'indicator': ['Account Ownership', 'Digital Payments', 'Telebirr Launch', None],
            'indicator_code': ['ACC_OWNERSHIP', 'USG_DIGITAL_PAYMENT', None, 'ACC_OWNERSHIP'],
            'value_numeric': [49.0, 35.0, None, 15.0],
            'observation_date': ['2024-01-01', '2024-01-01', None, None],
            'event_date': [None, None, '2021-05-01', None],
            'source_name': ['Global Findex', 'Global Findex', 'Ethio Telecom', 'Model'],
            'confidence': ['high', 'high', 'high', 'medium']
        })
        
        enriched_data.to_csv('data/processed/enriched_data.csv', index=False)
        print('Sample enriched data created')
        
        # Create sample forecast results
        forecast_results = pd.DataFrame({
            'indicator': ['Account Ownership', 'Digital Payments'],
            'year': [2027, 2027],
            'scenario': ['base', 'base'],
            'forecast_value': [55.0, 45.0],
            'unit': ['%', '%']
        })
        
        forecast_results.to_csv('models/forecast_results.csv', index=False)
        print('Sample forecast results created')
        "
    
    - name: Test forecasting module
      run: |
        python -c "
        import sys
        sys.path.append('src')
        try:
            from forecast_model import FinancialInclusionForecaster
            forecaster = FinancialInclusionForecaster('data/processed/enriched_data.csv')
            
            # Test data preparation
            try:
                data = forecaster.prepare_forecast_data('ACC_OWNERSHIP')
                print(f'âœ… Forecast data preparation successful')
                print(f'   Data points: {len(data)}')
            except Exception as e:
                print(f'âš ï¸  Forecast data preparation warning: {e}')
                
            print('âœ… Forecasting module loaded successfully')
            
        except Exception as e:
            print(f'âŒ Forecasting module failed: {e}')
            sys.exit(1)
        "
    
    - name: Upload coverage to Codecov
      uses: codecov/codecov-action@v3
      with:
        file: ./coverage.xml
        fail_ci_if_error: false
    
    - name: Archive test results
      if: always()
      uses: actions/upload-artifact@v3
      with:
        name: test-results-python-${{ matrix.python-version }}
        path: |
          htmlcov/
          test-results.xml
        retention-days: 7
    
    - name: Archive coverage reports
      if: always()
      uses: actions/upload-artifact@v3
      with:
        name: coverage-reports-python-${{ matrix.python-version }}
        path: |
          coverage.xml
        retention-days: 30

  integration-test:
    runs-on: ubuntu-latest
    needs: test
    if: github.event_name == 'pull_request' && github.base_ref == 'main'
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install streamlit pytest
    
    - name: Run integration tests
      run: |
        # Test end-to-end data pipeline
        echo "Running integration tests..."
        python -c "
        import sys
        sys.path.append('src')
        
        # Test complete pipeline
        print('Testing complete data pipeline...')
        
        try:
            # 1. Load data
            from data_loader import DataLoader
            loader = DataLoader()
            raw_data, ref_codes = loader.load_data()
            print('âœ… Step 1: Data loading successful')
            
            # 2. Enrich data
            from data_enricher import DataEnricher
            enricher = DataEnricher(raw_data)
            enricher.add_mobile_money_data()
            enriched_data = enricher.get_enriched_data()
            print(f'âœ… Step 2: Data enrichment successful (records: {len(enriched_data)})')
            
            # 3. Run EDA
            from eda_analyzer import EDAAnalyzer
            # Save enriched data temporarily for analyzer
            enriched_data.to_csv('temp_enriched.csv', index=False)
            analyzer = EDAAnalyzer('temp_enriched.csv')
            insights = analyzer.generate_key_insights()
            print(f'âœ… Step 3: EDA analysis successful (insights generated)')
            
            # 4. Test forecasting
            from forecast_model import FinancialInclusionForecaster
            forecaster = FinancialInclusionForecaster('temp_enriched.csv')
            forecasts = forecaster.forecast_all_indicators()
            print(f'âœ… Step 4: Forecasting successful ({len(forecasts)} indicators forecasted)')
            
            # Clean up
            import os
            if os.path.exists('temp_enriched.csv'):
                os.remove('temp_enriched.csv')
                
            print('ğŸ‰ All integration tests passed!')
            
        except Exception as e:
            print(f'âŒ Integration test failed: {e}')
            import traceback
            traceback.print_exc()
            sys.exit(1)
        "
    
    - name: Test dashboard can start
      run: |
        # Test that dashboard can be imported and initialized
        echo "Testing dashboard startup..."
        timeout 30s python -c "
        import sys
        import os
        sys.path.append('src')
        sys.path.append('dashboard')
        
        # Mock streamlit functions for testing
        import streamlit as st
        
        class MockStreamlit:
            def __init__(self):
                self.calls = []
            
            def __getattr__(self, name):
                def method(*args, **kwargs):
                    self.calls.append((name, args, kwargs))
                    return self
                return method
        
        # Replace real streamlit with mock for testing
        sys.modules['streamlit'] = MockStreamlit()
        import streamlit as st
        
        # Now import and test dashboard
        from app import FinancialInclusionDashboard
        
        try:
            dashboard = FinancialInclusionDashboard()
            print('âœ… Dashboard class instantiated successfully')
            
            # Test that it has required methods
            required_methods = ['render_overview', 'render_trends', 'render_forecasts', 'run']
            for method in required_methods:
                if hasattr(dashboard, method):
                    print(f'âœ… Dashboard has method: {method}')
                else:
                    print(f'âŒ Dashboard missing method: {method}')
                    
        except Exception as e:
            print(f'âŒ Dashboard test failed: {e}')
            raise
        "
      timeout-minutes: 2

  security-scan:
    runs-on: ubuntu-latest
    needs: test
    if: github.event_name == 'push' && github.ref == 'refs/heads/main'
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Run safety check
      run: |
        pip install safety
        safety check -r requirements.txt --full-report
    
    - name: Check for secrets in code
      uses: actions/checkout@v3
      with:
        fetch-depth: 0
    
    - name: Detect secrets
      uses: gitleaks/gitleaks-action@v2
      env:
        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      with:
        config-path: .gitleaks.toml

  documentation-check:
    runs-on: ubuntu-latest
    needs: test
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Check documentation completeness
      run: |
        echo "Checking documentation..."
        
        # Check README exists
        if [ ! -f "README.md" ]; then
          echo "âŒ README.md missing"
          exit 1
        fi
        
        # Check dashboard README
        if [ ! -f "dashboard/README.md" ]; then
          echo "âŒ dashboard/README.md missing"
          exit 1
        fi
        
        # Check notebooks have README
        if [ ! -f "notebooks/README.md" ]; then
          echo "âš ï¸  notebooks/README.md missing (warning only)"
        fi
        
        # Check for data dictionary
        if [ ! -f "docs/data_dictionary.md" ] && [ ! -f "data/README.md" ]; then
          echo "âš ï¸  Data dictionary missing (warning only)"
        fi
        
        # Check requirements.txt has descriptions
        if grep -q "^# " requirements.txt; then
          echo "âœ… requirements.txt has comments"
        else
          echo "âš ï¸  requirements.txt lacks comments (warning only)"
        fi
        
        echo "âœ… Documentation check complete"

  performance-test:
    runs-on: ubuntu-latest
    needs: test
    if: github.event_name == 'push' && github.ref == 'refs/heads/main'
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install memory_profiler psutil
    
    - name: Run performance tests
      run: |
        echo "Running performance tests..."
        
        # Create test data file
        python -c "
        import pandas as pd
        import numpy as np
        from datetime import datetime, timedelta
        
        # Create larger test dataset
        dates = pd.date_range(start='2011-01-01', end='2024-12-31', freq='M')
        n_indicators = 10
        n_records = len(dates) * n_indicators
        
        print(f'Creating performance test dataset with {n_records} records...')
        
        test_data = pd.DataFrame({
            'record_type': ['observation'] * n_records,
            'pillar': np.random.choice(['access', 'usage', 'infrastructure'], n_records),
            'indicator': np.random.choice([f'Indicator_{i}' for i in range(n_indicators)], n_records),
            'indicator_code': np.random.choice([f'IND_{i:03d}' for i in range(n_indicators)], n_records),
            'value_numeric': np.random.uniform(0, 100, n_records),
            'observation_date': np.repeat(dates, n_indicators),
            'source_name': ['Test'] * n_records,
            'confidence': np.random.choice(['high', 'medium', 'low'], n_records)
        })
        
        test_data.to_csv('data/processed/performance_test.csv', index=False)
        print('Test dataset created')
        "
    
    - name: Test data loading performance
      run: |
        python -m memory_profiler -o memory_usage.log python -c "
        import sys
        import time
        sys.path.append('src')
        
        from data_loader import DataLoader
        
        print('Testing data loading performance...')
        
        start_time = time.time()
        loader = DataLoader()
        
        # Test with performance dataset
        import pandas as pd
        data = pd.read_csv('data/processed/performance_test.csv')
        
        load_time = time.time() - start_time
        print(f'Data loading time: {load_time:.2f} seconds')
        print(f'Data size: {len(data)} records')
        print(f'Memory usage: {data.memory_usage(deep=True).sum() / 1024 / 1024:.2f} MB')
        
        if load_time > 5:
            print('âš ï¸  Warning: Data loading time exceeds 5 seconds')
        else:
            print('âœ… Data loading performance acceptable')
        "
    
    - name: Clean up test files
      if: always()
      run: |
        rm -f data/processed/performance_test.csv
        rm -f memory_usage.log

  deploy-check:
    runs-on: ubuntu-latest
    needs: [test, integration-test, security-scan]
    if: github.event_name == 'push' && github.ref == 'refs/heads/main'
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Deployment readiness check
      run: |
        echo "Checking deployment readiness..."
        
        # Check all required files exist
        required_files=(
          "requirements.txt"
          "dashboard/app.py"
          "src/__init__.py"
          "data/processed/enriched_data.csv"
          "models/forecast_results.csv"
        )
        
        all_files_exist=true
        for file in "${required_files[@]}"; do
          if [ ! -f "$file" ]; then
            echo "âŒ Missing required file: $file"
            all_files_exist=false
          fi
        done
        
        if [ "$all_files_exist" = true ]; then
          echo "âœ… All required files present"
        else
          echo "âŒ Missing required files"
          exit 1
        fi
        
        # Check Python syntax
        echo "Checking Python syntax..."
        python -m py_compile dashboard/app.py src/*.py 2>/dev/null || {
          echo "âŒ Python syntax errors found"
          exit 1
        }
        echo "âœ… Python syntax valid"
        
        echo "âœ… Project is ready for deployment"